{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Uploading multiple CSV files using SDK\n",
    "This tutorial will show you how to upload many CSV files using **intdash SDK for Python** (hereafter called intdash SDK).\n",
    "In this case, we will create one measurement for each CSV file. For the CSV file format, refer to the following \"Preparation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Preparation\n",
    "Before starting this scenario, prepare the following.\n",
    "\n",
    "- Edge for data upload\n",
    "- Multiple CSVs to upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data to be used\n",
    "In this tutorial, the following data needs to be ready on the server side.\n",
    "In this section, processing is performed with the following data names.\n",
    "\n",
    "| Data item | Data name that appears in this scenario |\n",
    "|:---|:---|\n",
    "| Edge to register time series data | edge1|\n",
    "| CSV that stores time series data| sampleX.csv (X = arbitrary number) |\n",
    "\n",
    "#### Details of CSV data to upload\n",
    "\n",
    "The CSV uploaded must meet the following conditions.\n",
    "* The first line must store the name of each column as a character string\n",
    "* The first column must contain the time stamp\n",
    "* Data must be stored in the second and subsequent columns.\n",
    "\n",
    "<img src=\"https://github.com/aptpod/aws-marketplace-tutorials/blob/master/img/img1.png?raw=true\\\">\n",
    "\n",
    "The column names given in the first line are used as the names of the data (`data_id` ).\n",
    "\n",
    "### Place CSV files to upload\n",
    "CSV files that store the time series data are placed in the `csv` directory under the same directory as this Jupyter Notebook. In this tutorial, the sample CSV files are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and create a client\n",
    "For `url` given to `intdash.Client`, specify the environment of the intdash server. For `username` and `password`, specify the auth information issued for the edge you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import intdash\n",
    "from intdash import timeutils\n",
    "\n",
    "# Create client\n",
    "client = intdash.Client(\n",
    "    url = \"https://example.intdash.jp\",\n",
    "    username = \"edge1\",\n",
    "    password=\"password_here\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preparations are complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Get the edge used to upload data\n",
    "First, get the edge to be used to upload CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = client.edges.list(name='edge1')\n",
    "edge1 = edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edge1'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load CSV files\n",
    "Load each CSV file as `pandas.DataFrame`. Here, the CSV files stored in the `csv/` directory are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "\n",
    "csv_files = glob.glob('./csv/*')\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    df = pd.read_csv(csv_path, index_col=0).groupby(\"time\").last()\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Create a measurement for each CSV file and register the data\n",
    "From here, we will focus on the processing for each CSV file. By repeating these processes, you can upload multiple CSV files.  \n",
    "\\* If you want to process multiple files immediately, skip this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick first DataFrame.\n",
    "df = dfs.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a measurement. Use the first timestamp of the data as the measurement start time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_measurements = client.measurements.create(\n",
    "    name='csv_data',  # Define name of measurement.\n",
    "    basetime=timeutils.str2timestamp(df.index[0]), #  Use timestamp of the first datapoint as the basetime of the measurement.\n",
    "    edge_uuid=edge1.uuid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the measurement, convert the DataFrame to `DataPoint` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = []\n",
    "\n",
    "for data_id, values in df.to_dict().items():\n",
    "    for time, value in values.items():\n",
    "        \n",
    "        if math.isnan(value) or value is '':\n",
    "            continue\n",
    "            \n",
    "        datapoints.append(\n",
    "            intdash.DataPoint(\n",
    "                elapsed_time= timeutils.str2timestamp(time) - new_measurements.basetime, # Time elapsed from the start of measurement.\n",
    "                data_type=intdash.DataType.float,\n",
    "                channel=1, # fixed at 1.\n",
    "                data_payload=intdash.data.Float(data_id=data_id, value=value).to_payload()\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the conversion is completed, associate the time series data with the measurement created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.data_points.store(\n",
    "    measurement_uuid=new_measurements.uuid,\n",
    "    data_points=datapoints\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the upload of one CSV data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Register multiple CSV files at once\n",
    "In the following, the processes in the previous section are combined into one. The same process is repeated for multiple DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    # Create a measurement.\n",
    "    new_measurements = client.measurements.create(\n",
    "        name='csv_data',  # Define name of measurement.\n",
    "        basetime=timeutils.str2timestamp(df.index[0]), #  Use timestamp of the first datapoint as the basetime of the measurement.\n",
    "        edge_uuid=edge1.uuid\n",
    "    )\n",
    "    \n",
    "    # Convert DataFrames to DataPoints.\n",
    "    datapoints = []\n",
    "\n",
    "    for data_id, values in df.to_dict().items():\n",
    "        for time, value in values.items():\n",
    "            \n",
    "            if math.isnan(value) or value is '':\n",
    "                continue\n",
    "\n",
    "            datapoints.append(\n",
    "                intdash.DataPoint(\n",
    "                    elapsed_time= timeutils.str2timestamp(time) - new_measurements.basetime, # Time elapsed from the start of measurement.\n",
    "                    data_type=intdash.DataType.float,\n",
    "                    channel=1, # fixed at 1.\n",
    "                    data_payload=intdash.data.Float(data_id=data_id, value=value).to_payload()\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    # Store datapoints.\n",
    "    client.data_points.store(\n",
    "        measurement_uuid=new_measurements.uuid,\n",
    "        data_points=datapoints\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By executing the above, you can upload multiple CSV files.\n",
    "Confirm that the new measurements are displayed in [Stored Data] of Visual M2M Data Visualizer.\n",
    "<img src=\"https://github.com/aptpod/aws-marketplace-tutorials/blob/master/img/img3.png?raw=true\\\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
