{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Processing the smartphone data in real time and sending back to the server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, the data acquired by **intdash Motion** is processed by the **intdash SDK for Python**  (hereinafter referred to as intdash SDK) and uploaded to intdash. As a sample of function processing, the moving average is applied to the acceleration data.\n",
    "\n",
    "In this case, we will focus on the following methods.\n",
    "\n",
    "- Retrieve real time data from another edge\n",
    "- Apply a function to the retrieved data\n",
    "- Upload the processed data in real time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Preparation\n",
    "\n",
    "Before starting this scenario, prepare the following.\n",
    "\n",
    "- An edge that performs measurement\n",
    "- intdash Motion Application\n",
    "- Signal definition for general sensor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data to be used\n",
    "In this scenario, the following data needs to be ready on the server side.\n",
    "\n",
    "| Data item | Data name that appears in this scenario |\n",
    "|:---|:---|\n",
    "|Edge for data acquisition|edge1|\n",
    "|Edge for data upload|edge2|\n",
    "| Signal definitions(\\*) | `sp_ACCX`, `sp_ACCY`, `sp_ACCZ`|\n",
    "\n",
    "(\\*) Same as the signal definition used in SDK tutorial [3. Saving data from the smartphone as CSV](../3_save-data-as-csv/3_save-data-as-csv.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and create a client\n",
    "For `url` given to `intdash.Client`, specify the environment of the intdash server. For `username` and `password`, specify the auth information issued for the edge you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import intdash\n",
    "from intdash import timeutils\n",
    "\n",
    "# Create client\n",
    "client = intdash.Client(\n",
    "    url = \"https://example.intdash.jp\",\n",
    "    username = \"edge1\",\n",
    "    password=\"password_here\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm that the signal definitions are registered\n",
    "Confirm that the signal definitions for this scenario are registered.  \n",
    "If it is not registered, see **(Option) Register the signal definitions** in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = client.signals.list(label='sp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sp_ACCX, sp_ACCY, sp_ACCZ, "
     ]
    }
   ],
   "source": [
    "for s in signals:\n",
    "    print(s.label,  end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Option) Register the signal definitions\n",
    "```\n",
    "Warning:\n",
    "  If the target signal definition has already been registered to the server, skip this step.\n",
    "```\n",
    "Use the same signal definition that you used in the SDK tutorial [3. Saving data from the smartphone as CSV](../3_save-data-as-csv/3_save-data-as-csv.ipynb).\n",
    "Execute the following file to register the signal definitions.\n",
    "\n",
    "[0_create-signal-general-sensor.ipynb](../0_create-signal-general-sensor/0_create-signal-general-sensor.ipynb)  \n",
    "\n",
    "Register only the signal definitions for \"acceleration\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Retrieve the edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge1 = client.edges.list(name='edge1')[0]\n",
    "edge2 = client.edges.list(name='edge2')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('edge1', 'edge2')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge1.name, edge2.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Create a Queue\n",
    "The process from download to upload is performed using a Queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "\n",
    "q = queue.Queue(maxsize=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Preparation of data retrieval (Downstream)\n",
    "Define a process to receive time series data from an edge through the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Create a request\n",
    "For ``src_edge_uuid``, specify the source edge. In this example, the edge ``edge1`` running intdash Motion is used.\n",
    "Specify the ``label`` name of the signal definition in `data_id` of `core.DataFilter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_specs = [\n",
    "        intdash.DownstreamSpec(\n",
    "            src_edge_uuid = edge1.uuid, \n",
    "            filters = [\n",
    "                 intdash.DataFilter(data_type=intdash.DataType.float.value, data_id='sp_ACCX',channel=1),  # Acceleration\n",
    "                 intdash.DataFilter(data_type=intdash.DataType.float.value, data_id='sp_ACCY',channel=1),  # Acceleration\n",
    "                 intdash.DataFilter(data_type=intdash.DataType.float.value, data_id='sp_ACCZ',channel=1),  # Acceleration\n",
    "            ],\n",
    "        ),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Define a function to be applied to the received data\n",
    "In this scenario, the data receiving side receives the time series data and adds it to the Queue as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the received time-series data to the Queue.\n",
    "def callback(unit):\n",
    "    try:\n",
    "        q.put_nowait(unit)\n",
    "    except queue.Full:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Preparation for data upload (Upstream)\n",
    "Define a function that processes the received data and sends the data back to the server as new time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 Create a request\n",
    "Specify the UUID of the uploading edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_specs = [\n",
    "        intdash.UpstreamSpec(\n",
    "            src_edge_uuid = edge2.uuid,\n",
    "        ),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 Define the process\n",
    "In this scenario, define the process that calculates the moving average and returns it to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The function to calculate moving average.\n",
    "def calc_ave(score, array, ave_num):\n",
    "    array.append(score)\n",
    "    if len(array) > ave_num:\n",
    "        array.popleft()\n",
    "  \n",
    "    return  np.sum(array)/ len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.3 Define an uploader function\n",
    "\n",
    "Define the following processes:\n",
    "\n",
    "- Get data from the Queue\n",
    "- Switch the process depending on the data type of time series data\n",
    "- Put data into the process\n",
    "- Send the newly created data back (return the intdash.Unit with \"yield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVE_NUM = 5\n",
    "\n",
    "import struct\n",
    "from collections import deque\n",
    "\n",
    "acc_x_dq = deque([])\n",
    "acc_y_dq = deque([])\n",
    "acc_z_dq = deque([])\n",
    "\n",
    "\n",
    "# Calculate moving average of the received time-series data, convert it to Unit and upload. \n",
    "def upload_func():\n",
    "    while True:\n",
    "        try:\n",
    "            unit = q.get_nowait()\n",
    "            \n",
    "            # Skip basetime.\n",
    "            if unit.data.data_type.value == intdash.DataType.basetime.value:\n",
    "                yield unit\n",
    "                continue\n",
    "                \n",
    "            if unit.data.data_type.value != intdash.DataType.float.value:\n",
    "                yield unit\n",
    "                continue\n",
    "                \n",
    "            # Get intdash.intdash.data.GeneralSensor.\n",
    "            sensor_data = unit.data\n",
    "                \n",
    "            if unit.data.data_id == 'sp_ACCX':\n",
    "                acc_x = unit.data.value\n",
    "                ave_acc_x = calc_ave(acc_x, acc_x_dq, AVE_NUM)\n",
    "                \n",
    "                if ave_acc_x is None:\n",
    "                    continue\n",
    "               \n",
    "                yield intdash.Unit(\n",
    "                      elapsed_time = unit.elapsed_time,\n",
    "                      channel = 1,\n",
    "                      data =  intdash.data.Float(data_id='sp_ACCX', value=ave_acc_x ),\n",
    "                    )\n",
    "                continue\n",
    "                \n",
    "                 \n",
    "            if unit.data.data_id == 'sp_ACCY':\n",
    "                acc_y = unit.data.value\n",
    "                ave_acc_y = calc_ave(acc_y, acc_y_dq, AVE_NUM)\n",
    "                \n",
    "                if ave_acc_y is None:\n",
    "                    continue\n",
    "               \n",
    "                yield intdash.Unit(\n",
    "                      elapsed_time = unit.elapsed_time,\n",
    "                      channel = 1,\n",
    "                      data =  intdash.data.Float(data_id='sp_ACCY', value=ave_acc_y ),\n",
    "                    )\n",
    "                continue\n",
    "                \n",
    "            if unit.data.data_id == 'sp_ACCZ':\n",
    "                acc_z = unit.data.value\n",
    "                ave_acc_z = calc_ave(acc_z, acc_z_dq, AVE_NUM)\n",
    "                \n",
    "                if ave_acc_z is None:\n",
    "                    continue\n",
    "                \n",
    "                yield intdash.Unit(\n",
    "                      elapsed_time = unit.elapsed_time,\n",
    "                      channel = 1,\n",
    "                      data = intdash.data.Float(data_id='sp_ACCZ', value=ave_acc_z ),\n",
    "                    )\n",
    "                \n",
    "                continue\n",
    "                \n",
    "        except queue.Empty:\n",
    "            yield"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Start stream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsconn = client.connect_websocket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 Start upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsconn.open_upstreams(\n",
    "    specs = u_specs,\n",
    "    iterators = [upload_func()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Start downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsconn.open_downstreams(\n",
    "    specs = d_specs,\n",
    "    callbacks = [callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Check data with Visual M2M Data Visualizer\n",
    "By using **Visual M2M Data Visualizer**, you can see that the data is being communicated in real time. If you import the \"SCREEN file (.scrn)\" and \"DAT file (.dat)\" stored in the same directory as this notebook into **Visual M2M Data Visualizer**, you can see the data as follows. See **\"Setting up Data Visualizer\" in intdash tutorial 2**.\n",
    " \n",
    " \n",
    "On the screen below, `Acceleration raw` panel shows the data before conversion (data sent by Motion app), and `Acceleration Converted` panel shows the moving average calculated using `intdash-py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/70192465/94385047-10d80900-017f-11eb-9c00-aae106b49aaa.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Disconnecting real-time processing\n",
    "If you want to end the process, be sure to execute the following to disconnect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsconn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
